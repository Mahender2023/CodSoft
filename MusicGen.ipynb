{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahender2023/CodSoft/blob/main/MusicGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e70e6dbb-3211-4ef9-93f6-efaba764ac77",
      "metadata": {
        "id": "e70e6dbb-3211-4ef9-93f6-efaba764ac77"
      },
      "source": [
        "## Prepare the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d1fb09-4e19-4e82-a4fa-eea7b20bb96c",
      "metadata": {
        "id": "04d1fb09-4e19-4e82-a4fa-eea7b20bb96c"
      },
      "source": [
        "Letâ€™s make sure weâ€™re connected to a GPU to run this notebook. To get a free Tier T4 GPU, click `Connect T4` in the top right-hand corner of the screen. If you have access to Colab Pro, you can select a more performant GPU by clicking `Runtime` -> `Change runtime type`, then change `Hardware accelerator` from `None` to your choice of GPU. We can verify that weâ€™ve been assigned a GPU and view its specifications through the `nvidia-smi` command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21d38c22-bb79-495c-8aa9-09ceabb2957a",
      "metadata": {
        "id": "21d38c22-bb79-495c-8aa9-09ceabb2957a"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1abcac4f-06b0-41c7-b7e4-960ddd297afd",
      "metadata": {
        "id": "1abcac4f-06b0-41c7-b7e4-960ddd297afd"
      },
      "source": [
        "We see here that we've got on Tesla T4 16GB GPU, although this may vary for you depending on GPU availablity and Colab GPU assignment.\n",
        "\n",
        "Next, we install the ðŸ¤— Transformers package from the main branch, as well as ðŸ¤— Datasets package to load audio files for audio-prompted generation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66af0411-c18e-4d8b-b6d9-318ff4460e48",
      "metadata": {
        "id": "66af0411-c18e-4d8b-b6d9-318ff4460e48"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet pip\n",
        "!pip install --upgrade --quiet transformers datasets[audio]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ee39cc-654b-4f0e-b601-013e484c16f0",
      "metadata": {
        "id": "77ee39cc-654b-4f0e-b601-013e484c16f0"
      },
      "source": [
        "## Load the Model\n",
        "\n",
        "The pre-trained MusicGen small, medium and large checkpoints can be loaded from the [pre-trained weights](https://huggingface.co/models?search=facebook/musicgen-) on the Hugging Face Hub. Change the repo id with the checkpoint size you wish to load. We'll default to the small checkpoint, which is the fastest of the three but has the lowest audio quality:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d87424-9f38-4658-ba47-2a465d52ad77",
      "metadata": {
        "id": "b0d87424-9f38-4658-ba47-2a465d52ad77"
      },
      "outputs": [],
      "source": [
        "from transformers import MusicgenForConditionalGeneration\n",
        "\n",
        "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4981d112-407c-4120-86aa-5c6a170543f7",
      "metadata": {
        "id": "4981d112-407c-4120-86aa-5c6a170543f7"
      },
      "source": [
        "We can then place the model on our accelerator device (if available), or leave it on the CPU otherwise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9508dee8-39df-46fe-82f3-6cc2e9f21a97",
      "metadata": {
        "id": "9508dee8-39df-46fe-82f3-6cc2e9f21a97"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6e1166e-1335-4555-9ec4-223d1fbcb547",
      "metadata": {
        "id": "f6e1166e-1335-4555-9ec4-223d1fbcb547"
      },
      "source": [
        "## Generation\n",
        "\n",
        "MusicGen is compatible with two generation modes: greedy and sampling. In practice, sampling leads to significantly\n",
        "better results than greedy, thus we encourage sampling mode to be used where possible. Sampling is enabled by default,\n",
        "and can be explicitly specified by setting `do_sample=True` in the call to `MusicgenForConditionalGeneration.generate` (see below).\n",
        "\n",
        "### Unconditional Generation\n",
        "\n",
        "The inputs for unconditional (or 'null') generation can be obtained through the method `MusicgenForConditionalGeneration.get_unconditional_inputs`. We can then run auto-regressive generation using the `.generate` method, specifying `do_sample=True` to enable sampling mode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb7708e8-e4f1-4ab8-b04a-19395d78dea2",
      "metadata": {
        "id": "fb7708e8-e4f1-4ab8-b04a-19395d78dea2"
      },
      "outputs": [],
      "source": [
        "unconditional_inputs = model.get_unconditional_inputs(num_samples=1)\n",
        "\n",
        "audio_values = model.generate(**unconditional_inputs, do_sample=True, max_new_tokens=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94cb74df-c194-4d2e-930a-12473b08a919",
      "metadata": {
        "id": "94cb74df-c194-4d2e-930a-12473b08a919"
      },
      "source": [
        "The audio outputs are a three-dimensional Torch tensor of shape `(batch_size, num_channels, sequence_length)`. To listen\n",
        "to the generated audio samples, you can either play them in an ipynb notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15f0bc7c-b899-4e7a-943e-594e73f080ea",
      "metadata": {
        "id": "15f0bc7c-b899-4e7a-943e-594e73f080ea"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "sampling_rate = model.config.audio_encoder.sampling_rate\n",
        "Audio(audio_values[0].cpu().numpy(), rate=sampling_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de58334-40f7-4924-addb-2d6ff34c0590",
      "metadata": {
        "id": "6de58334-40f7-4924-addb-2d6ff34c0590"
      },
      "source": [
        "Or save them as a `.wav` file using a third-party library, e.g. `scipy` (note here that we also need to remove the channel dimension from our audio tensor):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04291f52-0a75-4ddb-9eff-e853d0f17288",
      "metadata": {
        "id": "04291f52-0a75-4ddb-9eff-e853d0f17288"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "scipy.io.wavfile.write(\"musicgen_out.wav\", rate=sampling_rate, data=audio_values[0, 0].cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e52ff5b2-c170-4079-93a4-a02acbdaeb39",
      "metadata": {
        "id": "e52ff5b2-c170-4079-93a4-a02acbdaeb39"
      },
      "source": [
        "The argument `max_new_tokens` specifies the number of new tokens to generate. As a rule of thumb, you can work out the length of the generated audio sample in seconds by using the frame rate of the EnCodec model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d75ad107-e19b-47f3-9cf1-5102ab4ae74a",
      "metadata": {
        "id": "d75ad107-e19b-47f3-9cf1-5102ab4ae74a"
      },
      "outputs": [],
      "source": [
        "audio_length_in_s = 256 / model.config.audio_encoder.frame_rate\n",
        "\n",
        "audio_length_in_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fba4154-13f6-403a-958b-101d6eacfb6e",
      "metadata": {
        "id": "5fba4154-13f6-403a-958b-101d6eacfb6e"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
        "\n",
        "inputs = processor(\n",
        "    text=[\"80s pop track with bassy drums and synth\", \"90s rock song with loud guitars and heavy drums\"],\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "audio_values = model.generate(**inputs.to(device), do_sample=True, guidance_scale=3, max_new_tokens=256)\n",
        "\n",
        "Audio(audio_values[0].cpu().numpy(), rate=sampling_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "56a5c28a-f6c1-4ac8-ae08-6776a2b2c5b8",
      "metadata": {
        "id": "56a5c28a-f6c1-4ac8-ae08-6776a2b2c5b8",
        "outputId": "b6e5819c-0d14-40c4-9f99-e0d98ceedddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532,
          "referenced_widgets": [
            "320b80d10b704674a07d277ed0afc27b",
            "319d049c732044fd89eb2520dc1b7494",
            "5328598a98d14081bf5706ff48b95c23",
            "c69ef8c54cf0431791b718b5d99012da",
            "e8bc20a8be344ed2af2f4dbbb2a216b6",
            "24d05cdd877d4f6eb4f08036f113c0bb",
            "390d23d65978417aa230a5fe9b3e1651",
            "02bcc85770cf4e77a5aed52f49d81c16",
            "8b361954c587469dbedefd2606fbd518",
            "0cce2bdc438543a3936577f3e5bc1289",
            "773178908bd8446dbfd04d0be8cd60eb"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/703 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "320b80d10b704674a07d277ed0afc27b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Loading a streaming dataset cached in a LocalFileSystem is not supported yet.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-46b3057a66ff>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sanchit-gandhi/gtzan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreaming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"audio\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m     \u001b[0;31m# Return iterable dataset in case of streaming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstreaming\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_streaming_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m     \u001b[0;31m# Some datasets are already processed on the HF google storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36mas_streaming_dataset\u001b[0;34m(self, split, base_path)\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \u001b[0mis_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_remote_filesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m             raise NotImplementedError(\n\u001b[0m\u001b[1;32m   1319\u001b[0m                 \u001b[0;34mf\"Loading a streaming dataset cached in a {type(self._fs).__name__} is not supported yet.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Loading a streaming dataset cached in a LocalFileSystem is not supported yet."
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"sanchit-gandhi/gtzan\", split=\"train\", streaming=True)\n",
        "sample = next(iter(dataset))[\"audio\"]\n",
        "\n",
        "# take the first half of the audio sample\n",
        "sample[\"array\"] = sample[\"array\"][: len(sample[\"array\"]) // 2]\n",
        "\n",
        "inputs = processor(\n",
        "    audio=sample[\"array\"],\n",
        "    sampling_rate=sample[\"sampling_rate\"],\n",
        "    text=[\"80s blues track with groovy saxophone\"],\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "audio_values = model.generate(**inputs.to(device), do_sample=True, guidance_scale=3, max_new_tokens=256)\n",
        "\n",
        "Audio(audio_values[0].cpu().numpy(), rate=sampling_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5495f568-51ca-439d-b47b-8b52e89b78f1",
      "metadata": {
        "id": "5495f568-51ca-439d-b47b-8b52e89b78f1"
      },
      "outputs": [],
      "source": [
        "sample = next(iter(dataset))[\"audio\"]\n",
        "\n",
        "# take the first quater of the audio sample\n",
        "sample_1 = sample[\"array\"][: len(sample[\"array\"]) // 4]\n",
        "\n",
        "# take the first half of the audio sample\n",
        "sample_2 = sample[\"array\"][: len(sample[\"array\"]) // 2]\n",
        "\n",
        "inputs = processor(\n",
        "    audio=[sample_1, sample_2],\n",
        "    sampling_rate=sample[\"sampling_rate\"],\n",
        "    text=[\"80s blues track with groovy saxophone\", \"90s rock song with loud guitars and heavy drums\"],\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "audio_values = model.generate(**inputs.to(device), do_sample=True, guidance_scale=3, max_new_tokens=256)\n",
        "\n",
        "# post-process to remove padding from the batched audio\n",
        "audio_values = processor.batch_decode(audio_values, padding_mask=inputs.padding_mask)\n",
        "\n",
        "Audio(audio_values[0], rate=sampling_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation Config\n",
        "\n",
        "The default parameters that control the generation process, such as sampling, guidance scale and number of generated tokens, can be found in the model's generation config, and updated as desired. Let's first inspect the default generation config:"
      ],
      "metadata": {
        "id": "viwTDmzl8ZDN"
      },
      "id": "viwTDmzl8ZDN"
    },
    {
      "cell_type": "code",
      "source": [
        "model.generation_config"
      ],
      "metadata": {
        "id": "0zM4notb8Y1g"
      },
      "id": "0zM4notb8Y1g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright! We see that the model defaults to using sampling mode (`do_sample=True`), a guidance scale of 3, and a maximum generation length of 1500 (which is equivalent to 30s of audio). You can update any of these attributes to change the default generation parameters:"
      ],
      "metadata": {
        "id": "DLSnSwau8jyW"
      },
      "id": "DLSnSwau8jyW"
    },
    {
      "cell_type": "code",
      "source": [
        "# increase the guidance scale to 4.0\n",
        "model.generation_config.guidance_scale = 4.0\n",
        "\n",
        "# set the max new tokens to 256\n",
        "model.generation_config.max_new_tokens = 256\n",
        "\n",
        "# set the softmax sampling temperature to 1.5\n",
        "model.generation_config.temperature = 1.5"
      ],
      "metadata": {
        "id": "ensSj1IB81dA"
      },
      "id": "ensSj1IB81dA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-running generation now will use the newly defined values in the generation config:"
      ],
      "metadata": {
        "id": "UjqGnfc-9ZFJ"
      },
      "id": "UjqGnfc-9ZFJ"
    },
    {
      "cell_type": "code",
      "source": [
        "audio_values = model.generate(**inputs.to(device))"
      ],
      "metadata": {
        "id": "KAExrhDl9YvS"
      },
      "id": "KAExrhDl9YvS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that any arguments passed to the generate method will **supersede** those in the generation config, so setting `do_sample=False` in the call to generate will supersede the setting of `model.generation_config.do_sample` in the generation config."
      ],
      "metadata": {
        "id": "HdGdoGAs84hS"
      },
      "id": "HdGdoGAs84hS"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s__neSDH89q0"
      },
      "id": "s__neSDH89q0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "320b80d10b704674a07d277ed0afc27b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_319d049c732044fd89eb2520dc1b7494",
              "IPY_MODEL_5328598a98d14081bf5706ff48b95c23",
              "IPY_MODEL_c69ef8c54cf0431791b718b5d99012da"
            ],
            "layout": "IPY_MODEL_e8bc20a8be344ed2af2f4dbbb2a216b6"
          }
        },
        "319d049c732044fd89eb2520dc1b7494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24d05cdd877d4f6eb4f08036f113c0bb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_390d23d65978417aa230a5fe9b3e1651",
            "value": "Downloadingâ€‡readme:â€‡100%"
          }
        },
        "5328598a98d14081bf5706ff48b95c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02bcc85770cf4e77a5aed52f49d81c16",
            "max": 703,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b361954c587469dbedefd2606fbd518",
            "value": 703
          }
        },
        "c69ef8c54cf0431791b718b5d99012da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cce2bdc438543a3936577f3e5bc1289",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_773178908bd8446dbfd04d0be8cd60eb",
            "value": "â€‡703/703â€‡[00:00&lt;00:00,â€‡8.72kB/s]"
          }
        },
        "e8bc20a8be344ed2af2f4dbbb2a216b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24d05cdd877d4f6eb4f08036f113c0bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "390d23d65978417aa230a5fe9b3e1651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02bcc85770cf4e77a5aed52f49d81c16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b361954c587469dbedefd2606fbd518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0cce2bdc438543a3936577f3e5bc1289": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "773178908bd8446dbfd04d0be8cd60eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}